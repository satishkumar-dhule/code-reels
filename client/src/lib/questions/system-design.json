[
  {
    "id": "sd-1",
    "question": "Can you explain the Load Balancer strategy? When would you use Layer 4 vs Layer 7 load balancing?",
    "answer": "Load Balancing distributes traffic across multiple servers to ensure reliability and scalability.",
    "explanation": "A Load Balancer (LB) acts as a reverse proxy. \n\n**Layer 4 (Transport Layer)**: Distributes based on IP/Port. Fast, low overhead, but no context of content. Good for simple packet distribution.\n\n**Layer 7 (Application Layer)**: Inspects HTTP headers/content. Can route based on URL/cookies (e.g., /api to Service A, /static to Service B). More expensive but smarter.\n\n**Common Algorithms**:\n- **Round Robin**: Sequential.\n- **Least Connections**: Sends to server with fewest active connections.\n- **IP Hash**: Ensures a user always goes to the same server (sticky sessions).",
    "tags": [
      "infra",
      "scale",
      "networking"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "graph LR\n    User --> LB[Load Balancer]\n    LB -->|Layer 4| S1[\"Server 1<br/>IP:Port\"]\n    LB -->|Layer 7| S2[\"Server 2<br/>/api\"]\n    style LB fill:#fff,stroke:#000,color:#000",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sd-2",
    "question": "What is Consistent Hashing and why is it critical for distributed caches?",
    "answer": "Consistent Hashing maps keys to a ring of nodes to minimize data movement when scaling.",
    "explanation": "In standard `hash(key) % N`, adding a node changes `N`, causing nearly ALL keys to remap (cache stampede).\n\n**Consistent Hashing** maps both servers and keys to a circle (0-360Â°). Keys map to the next server clockwise.\n\n**Benefit**: Adding/removing a node only affects the immediate neighbors (k/N keys move), not the whole cluster.\n\nUsed in: DynamoDB, Cassandra, Discord Ringpop.",
    "tags": [
      "hashing",
      "dist-sys",
      "caching"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "distributed-systems",
    "diagram": "\ngraph TD\n    subgraph Hash Ring\n    N1((Node 1)) --- N2((Node 2))\n    N2 --- N3((Node 3))\n    N3 --- N1\n    end\n    Key[Key K] -.->|Clockwise| N2\n    style N2 fill:#f00,stroke:#fff,color:#fff\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sd-3",
    "question": "Explain the CAP Theorem. Can you really 'choose two'?",
    "answer": "CAP states a distributed store can only provide 2 of 3: Consistency, Availability, Partition Tolerance.",
    "explanation": "**Partition Tolerance (P)** is NOT optional in distributed systems (networks fail). \n\nSo the real choice is **CP vs AP** during a partition:\n\n- **CP (Consistency)**: Return error/timeout if data can't be synced. (e.g., Banking - better to fail than show wrong balance).\n- **AP (Availability)**: Return stale data but keep running. (e.g., Facebook Feed - better to show old posts than nothing).\n\n**PACELC Theorem** extends this: Else (when no partition), choose Latency (L) vs Consistency (C).",
    "tags": [
      "theory",
      "dist-sys",
      "database"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "distributed-systems",
    "diagram": "graph TD\n    CAP[CAP Theorem]\n    CAP --> C[Consistency]\n    CAP --> A[Availability]\n    CAP --> P[Partition Tolerance]\n    Note[Pick 2 of 3]\n    style Note fill:#f59e0b,stroke:#fff,color:#000",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sd-4",
    "question": "How do you handle Database Sharding? What are the downsides?",
    "answer": "Sharding splits a large database into smaller, faster, easily managed parts called data shards.",
    "explanation": "**Horizontal Partitioning**: Splitting rows based on a Shard Key (e.g., UserID).\n\n**Downsides/Challenges**:\n1. **Resharding**: Hard to move data when a shard fills up.\n2. **Hotspot Key**: If Justin Bieber is on Shard 1, Shard 1 melts down.\n3. **Joins**: Cross-shard joins are expensive/impossible.\n\n**Mitigation**: Consistent Hashing, Virtual Nodes.",
    "tags": [
      "db",
      "scale",
      "architecture"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "database",
    "diagram": "\ngraph TD\n    App --> Router\n    Router -->|ID < 100| S1[(Shard 1)]\n    Router -->|ID > 100| S2[(Shard 2)]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sd-5",
    "question": "Design a Rate Limiter. What algorithms would you consider?",
    "answer": "Rate Limiting controls the amount of traffic sent or received by a network interface controller.",
    "explanation": "Prevents DoS attacks and resource starvation.\n\n**Algorithms**:\n1. **Token Bucket**: Tokens added at rate `r`. Request consumes token. Allows bursts.\n2. **Leaky Bucket**: Requests enter queue, processed at constant rate. Smooths traffic.\n3. **Fixed Window**: Count requests in 1s window. Edge case: 2x traffic at window boundary.\n4. **Sliding Window Log**: Precise but expensive (stores timestamps).\n\n**Implementation**: Redis (Lua scripts for atomicity).",
    "tags": [
      "security",
      "api",
      "algorithms"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "api-design",
    "diagram": "\ngraph LR\n    Req[Request] --> Check{Buckets Full?}\n    Check -->|No| Process[Process]\n    Check -->|Yes| Drop[429 Too Many Requests]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-31",
    "question": "What is Scalability in DevOps?",
    "answer": "Scalability is the capability of a system to handle a growing amount of work by adding resources to the system. There are two types of scaling:",
    "explanation": "Scalability is the capability of a system to handle a growing amount of work by adding resources to the system. There are two types of scaling:\n\n1. **Vertical Scaling (Scale Up):**\n- Adding more power to existing resources\n- Example: Upgrading CPU/RAM\n\n2. **Horizontal Scaling (Scale Out):**\n- Adding more resources\n- Example: Adding more servers",
    "tags": [
      "scale",
      "ha"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "\ngraph TD\n    subgraph Vertical\n    S1[Small] --> S2[Large]\n    end\n    subgraph Horizontal\n    H1[Server] --- H2[Server] --- H3[Server]\n    end\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-32",
    "question": "What is High Availability?",
    "answer": "High Availability (HA) is a characteristic of a system that aims to ensure an agreed level of operational performance, usually uptime, for a higher th...",
    "explanation": "High Availability (HA) is a characteristic of a system that aims to ensure an agreed level of operational performance, usually uptime, for a higher than normal period.\n\nKey components:\n1. **Redundancy:**\n- Multiple instances\n- No single point of failure\n\n2. **Monitoring:**\n- Health checks\n- Automated failover\n\n3. **Load Balancing:**\n- Traffic distribution\n- Resource optimization",
    "tags": [
      "scale",
      "ha"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "\ngraph TD\n    LB[Load Balancer] --> S1[Server 1]\n    LB --> S2[Server 2]\n    S1 -.->|Failover| S2\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-33",
    "question": "What is Load Balancing?",
    "answer": "Load Balancing is the process of distributing network traffic across multiple servers to ensure no single server bears too much demand.",
    "explanation": "Load Balancing is the process of distributing network traffic across multiple servers to ensure no single server bears too much demand.\n\nCommon Load Balancing algorithms:\n1. **Round Robin**\n2. **Least Connections**\n3. **IP Hash**\n4. **Weighted Round Robin**\n5. **Resource-Based**\n\nExample of Nginx Load Balancer configuration:\n```nginx\nhttp {\nupstream backend {\nserver backend1.example.com;\nserver backend2.example.com;\nserver backend3.example.com;\n}\n\nserver {\nlisten 80;\nlocation / {\nproxy_pass http://backend;\n}\n}\n}\n```",
    "tags": [
      "scale",
      "ha"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "gh-34",
    "question": "What is Auto Scaling?",
    "answer": "Auto Scaling is a feature that automatically adjusts the number of compute resources based on the current demand.",
    "explanation": "Auto Scaling is a feature that automatically adjusts the number of compute resources based on the current demand.\n\nKey concepts:\n1. **Scaling Policies:**\n- Target tracking\n- Step scaling\n- Simple scaling\n\n2. **Metrics:**\n- CPU utilization\n- Memory usage\n- Request count\n- Custom metrics\n\nExample of AWS Auto Scaling configuration:\n```yaml\nAutoScalingGroup:\nMinSize: 1\nMaxSize: 10\nDesiredCapacity: 2\nHealthCheckType: ELB\nHealthCheckGracePeriod: 300\nLaunchTemplate:\nLaunchTemplateId: !Ref LaunchTemplate\nVersion: !GetAtt LaunchTemplate.LatestVersionNumber\n```",
    "tags": [
      "scale",
      "ha"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "infrastructure",
    "diagram": "\ngraph LR\n    Metrics[Metrics] --> ASG[Auto Scaling]\n    ASG -->|Scale Out| Add[Add Instances]\n    ASG -->|Scale In| Remove[Remove Instances]\n",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  },
  {
    "id": "sy-132",
    "question": "Design a distributed rate limiting system that can handle 1M+ requests per second across multiple data centers while maintaining consistency and low latency. How would you handle burst traffic, different rate limiting algorithms (token bucket, sliding window), and ensure fair distribution across users?",
    "answer": "Use distributed token bucket with Redis Cluster, consistent hashing for user distribution, and local caching with periodic sync for low latency.",
    "explanation": "## Distributed Rate Limiting System Design\n\n### Core Components\n\n**1. Rate Limiting Algorithms**\n- **Token Bucket**: Best for burst handling, allows temporary spikes\n- **Sliding Window**: More accurate but computationally expensive\n- **Fixed Window**: Simple but can cause boundary issues\n\n**2. Architecture Overview**\n- **API Gateway Layer**: First line of defense with local rate limiting\n- **Distributed Cache**: Redis Cluster for shared state across regions\n- **Rate Limit Service**: Dedicated microservice for complex logic\n- **Configuration Service**: Dynamic rule updates without deployment\n\n### Implementation Strategy\n\n**Local + Distributed Hybrid Approach:**\n```\n1. Local cache (99% of requests) - sub-millisecond latency\n2. Periodic sync with distributed store (every 100ms)\n3. Fallback to distributed check for edge cases\n```\n\n**Data Distribution:**\n- Consistent hashing for user â†’ shard mapping\n- Replication factor of 3 for high availability\n- Cross-region replication with eventual consistency\n\n**Handling Scale:**\n- Partition by user ID hash\n- Use Lua scripts in Redis for atomic operations\n- Implement circuit breakers for Redis failures\n- Local rate limiting as fallback\n\n### Advanced Features\n\n**Burst Handling:**\n- Token bucket with configurable burst capacity\n- Adaptive rate limiting based on system load\n- Priority queues for different user tiers\n\n**Fairness & Anti-Gaming:**\n- Per-user quotas with spillover pools\n- Detect and penalize abusive patterns\n- Implement jitter to prevent thundering herd\n\n**Monitoring & Observability:**\n- Real-time metrics on rate limit hits\n- Distributed tracing for debugging\n- Alerting on unusual traffic patterns",
    "tags": [
      "api",
      "rest"
    ],
    "difficulty": "advanced",
    "channel": "system-design",
    "subChannel": "api-design",
    "diagram": "graph TD\n    A[Client Requests] --> B[Load Balancer]\n    B --> C[API Gateway Cluster]\n    C --> D[Local Rate Limiter]\n    D --> E{Within Local Limit?}\n    E -->|Yes| F[Process Request]\n    E -->|No| G[Check Distributed Store]\n    G --> H[Redis Cluster]\n    H --> I[Rate Limit Service]\n    I --> J{Within Global Limit?}\n    J -->|Yes| K[Update Counters]\n    J -->|No| L[Reject Request]\n    K --> F\n    L --> M[Return 429]\n    \n    N[Config Service] --> O[Rate Limit Rules]\n    O --> C\n    O --> I\n    \n    P[Monitoring] --> Q[Metrics Collection]\n    Q --> R[Alerting]\n    \n    H --> S[Cross-Region Sync]\n    S --> T[Other Data Centers]",
    "lastUpdated": "2025-12-12T09:07:04.188Z"
  }
]